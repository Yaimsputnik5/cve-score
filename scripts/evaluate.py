#!/usr/bin/env python3

import json
import logging
import argparse
import settings
import numpy as np
from scipy import stats
from sklearn import model_selection
from core import utils

_RV_DISTRIBS = {
    'scipy.stats.expon': stats.expon,
}
_rv_factory = lambda params: _RV_DISTRIBS[params['class']](**params['args'])

_evaluator = None
_data = None
_rng = None

def build_rv_params(config):
    '''Unpacks the "param_distributions" object.'''
    return dict([
        (key, _rv_factory(value) if isinstance(value, dict) else value)
        for key, value in config.items()
    ])

def build_cv_params(config):
    '''Helper method to unmarshal a JSON-serializeable `config` object into
    constructor arguments for the RandomizedSearchCV class.
    '''
    xformers = {
        'estimator': utils.model_factory,
        'param_distributions': build_rv_params,
    }
    return dict([
        (key, xformers[key](value) if key in xformers else value)
        for (key, value) in config.items()
    ])

def split_data(test_size):
    '''Returns training/test split from the loaded data:
        (X_train, X_test, y_train, y_test)
    '''
    logging.info('splitting data, test_size: %s', test_size)
    return model_selection.train_test_split(
        *_data, test_size=test_size, random_state=_rng
    )

def main_loop(n_iter):
    for iter_ in range(1, n_iter+1):
        X_train, X_test, y_train, y_test = split_data(args.test_size)
        _evaluator.fit(X_train, y_train)
        logging.info('iter %d params %s', iter_, _evaluator.best_params_)
        scores = utils.scores(y_test, _evaluator.predict(X_test))
        logging.info('iter %d scores %s', iter_, scores)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('config', help='JSON file with CV config')
    parser.add_argument('features', help='sparse matrix file, npz format')
    parser.add_argument('labels', help='label vector file, numpy-serialized')
    parser.add_argument('--random-state', type=int, default=1337)
    parser.add_argument('--test-size', type=float, default=0.2,
        help='[float] rate of items in test/train split')
    parser.add_argument('--num-iter', type=int, default=5,
        help='[int] number of split/train/eval cycles')
    parser.add_argument('--logging', help='path to JSON config')
    args = parser.parse_args()

    settings.configure_logging(args.logging)

    config = json.load(open(args.config))
    params = build_cv_params(config)
    logging.info('using params %s', params)

    _evaluator = model_selection.RandomizedSearchCV(**params)
    _data = utils.load_data(vars(args))
    _rng = np.random.RandomState(args.random_state)
    main_loop(args.num_iter)
