#!/usr/bin/env python3

import argparse
import json
import logging

from collections import defaultdict

LOGGING_CFG = {
    'format': '%(asctime)s %(levelname)s %(message)s',
    'level': 'INFO'
}


def flat_map(stream, group_key, filter_keys=None):
    '''Transforms a stream of dictionaries by "flattening" along one key:

        {<key>: ["foo", "bar", "baz",], ...} =>
        {<key>: "foo", ...}, {<key>: "bar", ...}, {<key>: "baz", ...},

    Arguments:
        stream: iterator of dictionaries.
        group_key: key name to expand on.
        filter_keys: optional "whitelist" of key names to restrict the
            emitted dictionaries. Note: `group_key` is always included.

    Returns: iterator of dictionaries.
    '''
    for record in stream:
        items = [
            (key, value) for (key, value) in record.items()
            if filter_keys is not None and key in filter_keys
        ]
        group_values = record[group_key]
        if not isinstance(group_values, list):
            group_values = [group_values]
        for value in group_values:
            yield dict(items + [(group_key, value)])


def group_by(stream, group_key):
    '''Transforms a stream of dictionaries by "grouping by" one key.

        {<key>: "foo", "COL": "bar", ...}, {<key>: "foo", "COL": "baz", ...}
        => {<key>: "foo", "COL": ["bar", "baz"], ...}

    Arguments:
        stream: iterator of dictionaries.
        group_key: key name to aggreate by.

    Returns: iterator of dictionaries.
    '''
    index = defaultdict(lambda: defaultdict(list))
    for record in stream:
        node = record[group_key]
        for dest_key, value in record.items():
            if dest_key != group_key:
                index[node][dest_key].append(value)

    for node, record in index.items():
        record[group_key] = node
        yield record


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('infile', help='file of JSON records.')
    parser.add_argument('outfile', help='file of JSON records.')
    parser.add_argument('--group-key', required=True,
        help='key to be used in group aggregation.')
    parser.add_argument('--filter-keys', nargs='*')
    args = parser.parse_args()
    logging.basicConfig(**LOGGING_CFG)

    with open(args.infile) as f_in:
        logging.info('reading %s', args.infile)
        source = map(json.loads, f_in)
        flattened = flat_map(source, args.group_key, args.filter_keys)
        grouped = group_by(flattened, args.group_key)
        with open(args.outfile, 'w') as f_out:
            logging.info('writing %s', args.outfile)
            for record in grouped:
                f_out.write('{}\n'.format(json.dumps(record)))
