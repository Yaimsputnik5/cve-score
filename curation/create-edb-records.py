#!/usr/bin/env python3

import argparse
import csv
import json
import logging
import os


LOGGING_CFG = {
    'format': '%(asctime)s %(levelname)s %(message)s',
    'level': 'INFO'
}

SRC_FILE = 'files_exploits.csv'
SRC_FIELDS = ['edbid', 'file', 'title', 'date', 'author', 'type', 'platform']

_PREFIX = ''
TRANSFORMS = {
    'edbid': lambda token: token,
    'file': lambda token: os.path.join(_PREFIX, token),
    'title': lambda token: token,
    'date': lambda token: token,
    'type': lambda token: token,
    'platform': lambda token: token,
}


def transform_records(stream):
    '''Transforms a steam of CSV records to JSON serializable records.
    '''
    reader = csv.reader(stream)
    next(reader)  # skip header
    for raw_record in reader:
        yield {
            key: TRANSFORMS[key](value)
            for (key, value) in zip(SRC_FIELDS, raw_record)
            if key in TRANSFORMS
        }


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('prefix', help='path to exploit-db git repo')
    parser.add_argument('outfile', help='file of JSON records')
    args = parser.parse_args()
    logging.basicConfig(**LOGGING_CFG)

    _PREFIX = args.prefix
    source = os.path.join(_PREFIX, SRC_FILE)
    with open(source) as f_in:
        logging.info('reading %s', source)
        with open(args.outfile, 'w') as f_out:
            logging.info('writing %s', args.outfile)
            for record in transform_records(f_in):
                f_out.write('{}\n'.format(json.dumps(record)))
