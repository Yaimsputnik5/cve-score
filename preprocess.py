#!/usr/bin/env python3

import sys
import json
import logging
import argparse
import settings
import transforms
from collections import defaultdict, Counter

METHODS = {
    'flatten': transforms.flatten,
    'tokenize': transforms.tokenize,
    'identity': lambda value: value or [],
}
SUMMARY_METHODS = {'flatten', 'tokenize'}

_summaries = defaultdict(Counter)

def do_task(data, key, method):
    '''Performs a single preprocessing task.

    Arguments:
        data:   [dict] deserialized JSON object.
        key:    [str] top-level key in data to apply transform method.
        method: [str] name of method to apply to target value.

    Returns: (<name>, <result>) pair.
    '''
    result = METHODS[method](data.get(key))
    if method in SUMMARY_METHODS:
        global _summaries
        _summaries[key].update(result)
    return (key, result)

def emitter(stream, tasks):
    '''Iterator returning all transformed records.

    Arguments:
        stream: [file] stream of serialized JSON objects, one per line
        tasks:  [list] preprocessing task definitions
    '''
    for record in map(json.loads, stream):
        try:
            yield dict([do_task(record, **task) for task in tasks])
        except Exception as ex:
            logging.warn('%s => %s', record, ex)

def _xform_summary(summary):
    '''Unpacks a word-count structure into a list of dictionaries of the form:
        {"token": ..., "count": ...}
    '''
    items = sorted(summary.items(), key=lambda item: item[1], reverse=True)
    return [dict(token=token, count=count) for (token, count) in items]

def write_summaries(filename):
    '''Writes the count summaries of feature "keys" to target file.'''
    xformed = {key: _xform_summary(data) for (key, data) in _summaries.items()}
    with open(filename, 'w') as fh:
        logging.info('writing %s', filename)
        json.dump(xformed, fh)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('tasks', help='JSON file of task definitions')
    parser.add_argument('--infile',
        type=argparse.FileType('r'), default=sys.stdin, help='JSONL [stdin]')
    parser.add_argument('--outfile',
        type=argparse.FileType('w'), default=sys.stdout, help='JSONL [stdout]')
    parser.add_argument('--summary',
        help='filename (JSON) to emit word-counts')
    parser.add_argument('--logging', help='JSON file for basicConfig')
    args = parser.parse_args()

    settings.configure_logging(args.logging)
    with open(args.tasks) as fh:
        logging.info('loading %s', args.tasks)
        tasks = json.load(fh)

    for record in emitter(args.infile, tasks):
        args.outfile.write('%s\n' % json.dumps(record))

    if args.summary:
        write_summaries(args.summary)
