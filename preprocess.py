#!/usr/bin/env python3

import argparse
import json
import logging
import settings
import transforms
from collections import defaultdict, Counter

LOGGER = logging.getLogger('cve-score')

METHODS = {
    'flatten': transforms.flatten,
    'tokenize': transforms.tokenize,
    'identity': lambda value: value or [],
    'listcves': transforms.parse_cves,
    'binarize': lambda value: int(bool(value))
}

_RECORD_COUNT = 0
_TOKEN_COUNTS = defaultdict(Counter)


def transform(data, key, preprocessor, **kwargs):
    '''Performs a single preprocessing transform.

    Arguments:
        data: deserialized JSON argument.
        key:  top-level keyname in `data` to apply transform.
        preprocessor: name of transform in `METHODS`.

    Returns: (<key>, <result>) pair.
    '''
    result = METHODS[preprocessor](data.get(key))
    if isinstance(result, list):
        global _RECORD_COUNT, _TOKEN_COUNTS
        _RECORD_COUNT += 1
        _TOKEN_COUNTS[key].update(set(result))
    return (key, result)


def emitter(stream, config):
    '''Iterator returning all transformed records.

    Arguments:
        stream: stream of JSON records, one per line
        config: list of processing definitions
    '''
    for record in map(json.loads, stream):
        try:
            yield dict([transform(record, **item) for item in config])
        except Exception as ex:
            LOGGER.warn('%s => %s', record, ex)


def vocabulary():
    '''Returns a dictionary associating token-frequency mappings to their
    respective keynames.
    '''
    return {
        key: {
            token: (count / _RECORD_COUNT)
            for (token, count) in summary.items()
        }
        for (key, summary) in _TOKEN_COUNTS.items()
    }


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('config', help='JSON processing config')
    parser.add_argument('infile', help='raw JSON records, one per line')
    parser.add_argument('outfile')
    parser.add_argument('--vocabulary',
        help='target JSON file of token-frequency mappings')
    parser.add_argument('--logging', help='JSON logging config')
    args = parser.parse_args()

    settings.configure_logging(args.logging)
    with open(args.config) as fh:
        LOGGER.info('loading %s', args.config)
        config = json.load(fh)

    with open(args.infile) as f_in:
        with open(args.outfile, 'w') as f_out:
            LOGGER.info('writing %s', args.outfile)
            for record in emitter(f_in, config):
                f_out.write('%s\n' % json.dumps(record))

    if args.vocabulary:
        with open(args.vocabulary, 'w') as fh:
            LOGGER.info('writing %s', args.vocabulary)
            json.dump(vocabulary(), fh)
