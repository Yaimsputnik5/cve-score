#!/usr/bin/env python3

import json
import logging
import argparse
import settings
import transforms
from collections import defaultdict, Counter

METHODS = {
    'flatten': transforms.flatten,
    'tokenize': transforms.tokenize,
    'identity': lambda value: value or [],
}

SUMMARY_METHODS = {'flatten', 'tokenize'}
_summaries = defaultdict(Counter)

def _walk_object(data, keys):
    if not keys:
        return data
    elif isinstance(data, dict):
        child = data.get(keys[0], {})
        return _walk_object(child, keys[1:])
    else:
        raise Exception('path mismatch %s: %s' % (keys, data))

def do_task(data, path, method):
    '''Performs a single preprocessing task.

    Arguments:
        data:   [dict] deserialized JSON object.
        path:   [str] dot-separated path to target value.
        method: [str] name of method to apply to target value.

    Returns: (<name>, <result>) pair.
    '''
    target = _walk_object(data, path.split('.'))
    response = METHODS[method](target)
    if method in SUMMARY_METHODS:
        global _summaries
        _summaries[path].update(response)
    return (path, METHODS[method](target))

def emitter(stream, tasks):
    '''Iterator returning all transformed records.

    Arguments:
        stream: [file] stream of serialized JSON objects, one per line
        tasks:  [list] preprocessing task definitions
    '''
    for record in map(json.loads, stream):
        try:
            yield dict([do_task(record, **task) for task in tasks])
        except Exception as ex:
            logging.warn('%s => %s', record, ex)

def _xform_summary(summary):
    '''Unpacks a word-count structure into a list of dictionaries of the form:
        {"token": ..., "count": ...}
    '''
    items = sorted(summary.items(), key=lambda item: item[1], reverse=True)
    return [dict(token=token, count=count) for (token, count) in items]

def write_summaries(filename):
    '''Writes the count summaries of feature (paths) to target file.'''
    xformed = {path: _xform_summary(data) for (path, data) in _summaries.items()}
    with open(filename, 'w') as fh:
        logging.info('writing %s', filename)
        json.dump(xformed, fh)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('tasks', help='JSON file of task definitions')
    parser.add_argument('infile', help='JSONL file of raw records')
    parser.add_argument('outfile', help='JSONL file transfomred by "tasks"')
    parser.add_argument('summaries',
        help='JSON file to emit word-count summaries')
    parser.add_argument('--logging', help='JSON file for basicConfig')
    args = parser.parse_args()

    settings.configure_logging(args.logging)
    tasks = json.load(open(args.tasks))
    with open(args.infile) as fh_in:
        logging.info('reading %s', args.infile)
        with open(args.outfile, 'w') as fh_out:
            logging.info('writing %s', args.outfile)
            for record in emitter(fh_in, tasks):
                fh_out.write('%s\n' % json.dumps(record))

    write_summaries(args.summaries)
