#!/usr/bin/env python3

import json
import logging
import argparse
import settings
import transforms

METHODS = {
    'flatten': transforms.flatten,
    'tokenize': transforms.tokenize,
    'identity': lambda value: value,
}

def _walk_object(data, keys):
    if not keys:
        return data
    elif isinstance(data, dict):
        child = data.get(keys[0], {})
        return _walk_object(child, keys[1:])
    else:
        raise Exception('path mismatch %s: %s' % (keys, data))

def do_task(data, path, method):
    '''Performs a single preprocessing task.

    Arguments:
        data:   [dict] deserialized JSON object.
        path:   [str] dot-separated path to target value.
        method: [str] name of method to apply to target value.

    Returns: (<name>, <result>) pair.
    '''
    target = _walk_object(data, path.split('.'))
    return (path, METHODS[method](target))

def emitter(stream, tasks):
    '''Iterator returning all transformed records.

    Arguments:
        stream: [file] stream of serialized JSON objects, one per line
        tasks:  [list] preprocessing task definitions
    '''
    for record in map(json.loads, stream):
        try:
            yield dict([do_task(record, **task) for task in tasks])
        except Exception as ex:
            logging.warn('%s => %s', record, ex)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('tasks', help='JSON file of task definitions')
    parser.add_argument('infile')
    parser.add_argument('outfile')
    parser.add_argument('--logging', help='JSON file for basicConfig')
    args = parser.parse_args()

    settings.configure_logging(args.logging)
    tasks = json.load(open(args.tasks))
    with open(args.infile) as fh_in:
        logging.info('reading %s', args.infile)
        with open(args.outfile, 'w') as fh_out:
            logging.info('writing %s', args.outfile)
            for record in emitter(fh_in, tasks):
                fh_out.write('%s\n' % json.dumps(record))
