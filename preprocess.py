#!/usr/bin/env python3

import sys
import json
import logging
import argparse
import settings
import transforms
from collections import defaultdict, Counter

METHODS = {
    'flatten': transforms.flatten,
    'tokenize': transforms.tokenize,
    'identity': lambda value: value or [],
    'listcves': transforms.parse_cves,
    'binarize': lambda value: int(bool(value))
}
SUMMARY_METHODS = {'flatten', 'tokenize', 'listcves'}

_DOC_COUNTS = defaultdict(Counter)
_TERM_COUNTS = defaultdict(Counter)


def do_task(data, key, method):
    '''Performs a single preprocessing task.

    Arguments:
        data:   [dict] deserialized JSON object.
        key:    [str] top-level key in data to apply transform method.
        method: [str] name of method to apply to target value.

    Returns: (<key>, <result>) pair.
    '''
    result = METHODS[method](data.get(key))
    if method in SUMMARY_METHODS:
        global _DOC_COUNTS, _TERM_COUNTS
        _DOC_COUNTS[key].update(set(result))
        _TERM_COUNTS[key].update(result)
    return (key, result)

def emitter(stream, tasks):
    '''Iterator returning all transformed records.

    Arguments:
        stream: [file] stream of JSON records, one per line
        tasks:  [list] preprocessing task definitions
    '''
    for record in map(json.loads, stream):
        try:
            yield dict([do_task(record, **task) for task in tasks])
        except Exception as ex:
            logging.warn('%s => %s', record, ex)

def _join_counts(key):
    '''Unpacks a count structures into a list of dictionaries of the form:
        {"token": ..., "docCount": ..., "termCount": ...}
    '''
    docs, terms = _DOC_COUNTS[key], _TERM_COUNTS[key]
    return [
        dict(token=token, docCount=count, termCount=terms[token])
        for (token, count) in docs.items()
    ]

def write_summaries(filename):
    '''Writes the count summaries of feature "keys" to target file.'''
    xformed = {key: _join_counts(key) for key in _DOC_COUNTS}
    with open(filename, 'w') as fh:
        logging.info('writing %s', filename)
        json.dump(xformed, fh)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('tasks', help='JSON file of task definitions')
    parser.add_argument('--infile',
        type=argparse.FileType('r'), default=sys.stdin, help='JSONL [stdin]')
    parser.add_argument('--outfile',
        type=argparse.FileType('w'), default=sys.stdout, help='JSONL [stdout]')
    parser.add_argument('--summary',
        help='filename (JSON) to emit token stats')
    parser.add_argument('--logging', help='JSON file for basicConfig')
    args = parser.parse_args()

    settings.configure_logging(args.logging)
    with open(args.tasks) as fh:
        logging.info('loading %s', args.tasks)
        tasks = json.load(fh)

    for record in emitter(args.infile, tasks):
        args.outfile.write('%s\n' % json.dumps(record))

    if args.summary:
        write_summaries(args.summary)
