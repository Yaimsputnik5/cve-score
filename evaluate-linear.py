#!/usr/bin/env python3

import argparse
import logging
import shutil
import json

import numpy as np
import tensorflow as tf

LOGGER = logging.getLogger()

BUFFER_SIZE = 10000

LABEL_KEY = 'exploitdb'
FEATURE_KEY = 'cvssV2'
VOCABULARY = [
    "accessComplexity:HIGH",
    "accessComplexity:LOW",
    "accessComplexity:MEDIUM",
    "accessVector:ADJACENT_NETWORK",
    "accessVector:LOCAL",
    "accessVector:NETWORK",
    "authentication:MULTIPLE",
    "authentication:NONE",
    "authentication:SINGLE",
    "availabilityImpact:COMPLETE",
    "availabilityImpact:NONE",
    "availabilityImpact:PARTIAL",
    "confidentialityImpact:COMPLETE",
    "confidentialityImpact:NONE",
    "confidentialityImpact:PARTIAL",
    "integrityImpact:COMPLETE",
    "integrityImpact:NONE",
    "integrityImpact:PARTIAL"
]
LOOKUP = {token: i for (i, token) in enumerate(VOCABULARY)}


def encode(records):
    '''Returns FEATURES, LABELS pair from stream of JSON records.'''

    features, labels = [], []
    for record in map(json.loads, records):
        features.append([
            LOOKUP[token] for token in record[FEATURE_KEY] if token in LOOKUP
        ])
        labels.append(int(bool(record[LABEL_KEY])))

    return features, labels


def generator():
    '''Returns tensors generated from FEATURES, LABELS lists.'''
    cols = range(len(VOCABULARY))
    for row_ptr in range(len(FEATURES)):
        indices = set(FEATURES[row_ptr])
        feature = [(1.0 if j in indices else 0.0) for j in cols]
        label = LABELS[row_ptr]

        yield (feature, label)


def data_iterator(batch_size, shuffle=False):

    ds = tf.data.Dataset.from_generator(
        generator,
        (tf.float32, tf.int64),
        (tf.TensorShape([len(VOCABULARY)]), tf.TensorShape([])))

    if shuffle:
        ds = ds.shuffle(BUFFER_SIZE)

    ds = ds.batch(batch_size)
    iter_ = ds.make_one_shot_iterator()
    return iter_.get_next()


def model_fn(features, labels, mode):
    '''For the target Estimator.'''

    logits = tf.layers.dense(
        features, units=2,
        kernel_initializer=tf.truncated_normal_initializer())

    predictions = {
        'classes': tf.argmax(input=logits, axis=1),
        'probs': tf.nn.softmax(logits, name='softmax_tensor')
    }

    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

    onehot_labels = tf.one_hot(labels, depth=2)
    loss = tf.losses.softmax_cross_entropy(onehot_labels, logits)

    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.GradientDescentOptimizer(
            learning_rate=FLAGS.learning_rate)
        train_op = optimizer.minimize(
            loss=loss,
            global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(
            mode=mode, loss=loss, train_op=train_op)

    eval_metric_ops = {
        'accuracy': tf.metrics.accuracy(
            labels=labels, predictions=predictions['classes']),
    }
    return tf.estimator.EstimatorSpec(
        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)


class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)

        return json.JSONEncoder.default(self, obj)

def main(_):
    shutil.rmtree(FLAGS.model_dir, ignore_errors=True)

    estimator = tf.estimator.Estimator(
        model_fn=model_fn,
        model_dir=FLAGS.model_dir)

    for epoch in range(FLAGS.num_epochs):

        estimator.train(lambda: data_iterator(FLAGS.batch_size, True))

        results = estimator.evaluate(
            lambda: data_iterator(FLAGS.batch_size))

        results.update(epoch=epoch+1)
        print(NumpyEncoder().encode(results))


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('train', help='JSONL training examples')
    # parser.add_argument('eval', help='JSONL validation examples')
    parser.add_argument('--model-dir', default='/tmp/cve-softmax')
    parser.add_argument('--num-epochs', type=int, default=1)
    parser.add_argument('--batch-size', type=int, default=64)
    parser.add_argument('--learning-rate', type=float, default=1e-2)
    FLAGS = parser.parse_args()

    logging.basicConfig(
        format='%(asctime)s %(name)s %(levelname)s %(message)s',
        level='DEBUG')

    with open(FLAGS.train) as fh:
        FEATURES, LABELS = encode(fh)

    tf.app.run()
