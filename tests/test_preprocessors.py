import unittest
from preprocessors import Binarizer
from preprocessors import FlatMap
from preprocessors import Tokenizer


class TestBinarizer(unittest.TestCase):

    def test_correctness(self):
        # pairs of (<input>, <expected_output>)
        fixtures = [
            ('token',       1),
            ([2, 'vals'],   1),
            ({},            0),
            (None,          0),
        ]

        processor = Binarizer()
        for datum, expected in fixtures:
            self.assertEqual(expected, processor(datum))


class TestFlatMap(unittest.TestCase):

    def test_nested_objects(self):
        # pairs of (<input>, <expected_output>)
        fixtures = [
            ({'A': {'a': '1', 'b': 1}},     ['A.a:1', 'A.b:1']),
            ({'A': ['a', 'b']},             ['A:a', 'A:b']),
            (['A', {'B': 'b'}],             ['A', 'B:b']),
        ]

        processor = FlatMap()
        for datum, expected in fixtures:
            actual = processor(datum)
            self.assertEqual(sorted(expected), sorted(actual))

    def test_non_nested_objects(self):
        # pairs of (<input>, <expected_output>)
        fixtures = [
            (['A', 'B'],    ['A', 'B']),
            ('A',           ['A']),
            (1,             ['1']),
        ]

        processor = FlatMap()
        for datum, expected in fixtures:
            actual = processor(datum)
            self.assertEqual(sorted(expected), sorted(actual))


class TestTokenizer(unittest.TestCase):

    def test_regex_matcher(self):

        regex = '[\\w\\-]+'  # matches words with hyphens
        fixtures = [
            (
                'Hypenated-strings are  preserved!',
                ['hypenated-strings', 'are', 'preserved'],
            ),
            (
                'Tokens with inter=punc/tua/tion 4R3 split',
                ['tokens', 'with', 'inter', 'punc', 'tua',
                    'tion', '4r3', 'split'],
            )
        ]

        processor = Tokenizer(regex)
        for datum, expected in fixtures:
            self.assertEqual(expected, processor(datum))

    def test_list_handling(self):

        regex = '\\w+'
        fixtures = [
            (
                [
                    'Some first string,',
                    '-- and a second -- are handled!'
                ],
                [
                    'some', 'first', 'string', 'and', 'a', 'second',
                    'are', 'handled'
                ],
            ),
        ]

        processor = Tokenizer(regex)
        for datum, expected in fixtures:
            self.assertEqual(expected, processor(datum))
