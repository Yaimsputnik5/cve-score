import unittest
from transforms import flatten
from transforms import strip_punctuation
from transforms import tokenize


class TestMethods(unittest.TestCase):

    def test_flatten_correctness(self):

        input_expected_pairs = [
            (
                {'A': {'a': '1', 'b': 1}},
                ['A.a:1', 'A.b:1']
            ),
            (
                {'A': ['a', 'b']},
                ['A:a', 'A:b']
            ),
            (
                [{'A': 'a'}, {'A': 'b'}],
                ['A:a', 'A:b']
            ),
            (
                ['idem', 'potent'],
                ['idem', 'potent']
            ),
            (
                'singleton',
                ['singleton']
            )
        ]

        for input_, expected in input_expected_pairs:
            actual = flatten(input_)
            self.assertEqual(sorted(expected), sorted(actual))

    def test_strip_punctuation_correctness(self):

        input_expected_pairs = [
            (
                '#!/foo', 'foo'
            ),
            (
                'f-00', 'f-00'
            ),
            (
                '"foo,"', 'foo'
            )
        ]

        for input_, expected in input_expected_pairs:
            actual = strip_punctuation(input_)
            self.assertEqual(expected, actual)

    def test_tokenize_sanity(self):

        input_expected_pairs = [
            (
                ' Simple. Tokenization. Task! ',
                ['simple', 'tokenization', 'task']
            ),
        ]

        for input_, expected in input_expected_pairs:
            actual = tokenize(input_)
            self.assertEqual(expected, actual)
