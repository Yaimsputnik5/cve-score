#!/usr/bin/env python3

import argparse
import json
import logging
import pickle
import sys

from collections import defaultdict
import numpy as np
from scipy import sparse

from settings import configure_logging

LOGGER = logging.getLogger('cve-score')


def _bow_transform(vocabulary, tokens):
    '''Creates bag-of-words encoding from a list of tokens.

    Arguments:
        vocabulary: [list] defines "columns" of encoding.
        tokens:     [list] tokens to be encoded.

    Returns: `scipy.csr_matrix` of shape (1, N) where `N = len(vocabulary)`.    '''
    tokens = set(tokens)
    cind = [col for (col, word) in enumerate(vocabulary) if word in tokens]
    data = np.array([1.0] * len(cind), dtype=np.float32)
    return sparse.csr_matrix(
        (data, ([0] * len(cind), cind)),   # (data, (row_ind, col_ind)) CTOR
        shape=(1, len(vocabulary))
    )


def _label_transform(value):
    return np.array([value], dtype=np.int64)


def _weight_transform(value):
    return np.array([value or 1.0], dtype=np.float32)


def _get_transform(params):
    '''Converts a encoding parameters into a transformation.'''
    if 'vocabulary' in params:
        return lambda tokens: _bow_transform(params['vocabulary'], tokens)
    if params.get('isLabel', False):
        return _label_transform
    if params['key'] == 'weight':
        return _weight_transform

    raise ValueError('invalid encoding params %s' % params)


def numpy_encoder(config, records):
    '''Coverts a stream of JSON records into arrays of numpy objects. This
    is an "preprocessing" step serving downstream serialization methods,
    like pickling or protobuffers.

    Arguments:
        config  [list] list of dictionaries, each defining an encoder.
        records [iter] JSON serialized input, one record per line.

    Returns: [dict] mapping keys (specified in config objects) to lists of
        `numpy` or `scipy.sparse` objects.
    '''
    transforms =[
        (params['key'], _get_transform(params)) for params in config
    ]
    LOGGER.debug('transforms => %s', transforms)

    staged = defaultdict(list)
    for record in map(json.loads, records):
        for (key, xform) in transforms:
            staged[key].append(xform(record.get(key)))

    return dict(staged)


def _vstack(data):
    '''Adapter method for numpy/scipy.sparse operation.'''
    if isinstance(data[0], sparse.spmatrix):
        return sparse.vstack(data)
    return np.vstack(data).reshape((len(data),))


def do_pickle(filename, encoded):
    '''Serializes dictionary of numpy/scipy objects to a file.

    Arguments:
        filename  [str] target file.
        encoded   [dict] return object from `numpy_encoder`
    '''
    xformed = {key: _vstack(data) for (key, data) in encoded.items()}
    with open(filename, 'wb') as fh:
        LOGGER.info('writing %s', filename)
        pickle.dump(xformed, fh)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('config', help='JSON file for encodings')
    parser.add_argument('--infile',
        type=argparse.FileType('r'),
        default=sys.stdin,
        help='JSON records, one per line [stdin]')
    parser.add_argument('--pickle', help='Filename for numpy/scipy data')
    args = parser.parse_args()
    configure_logging()

    with open(args.config) as fh:
        config = json.load(fh)

    encoded = numpy_encoder(config, args.infile)

    if args.pickle:
        do_pickle(args.pickle, encoded)
