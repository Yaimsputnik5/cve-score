## Walk-through: CVSS and exploitability

This analysis requires a JSONL file of training examples like the one
generated in [curation/ReadMe.md](curation/ReadMe.md). It expects JSON records
to all have a property named "cvssV2" (the feature documents) and one named
"exploitdb" (the labels), but it carries over in a straightforward way
for any combination of features and labels.

### Step 1. Preprocessing the features

The script preprocess.py operates on JSONL streams to emit records that are
tokenized as part of a feature engineering step. The only required argument
is a config file of "tasks" which specifies the tokenization methods and
the "subdocuments" to which they are applied. The example file
[config/baseline/preproc.json](config/baseline/preproc.json)
illustrates this:

    $ ./preprocess.py config/baseline/preproc.json \
        --infile ~/nvd-data/nvd-edb-merged.jsonl \
        --outfile ~/nvd-data/nvd-edb-tokens.jsonl
    $ head ~/nvd-data/nvd-edb-tokens.jsonl |jq .

See the `METHODS` property of preprocess.py for available methods.

### Training/test split

Obtaining a realistic performance metric for a model requires evaluating it
on data that is held out from the training set. The approach taken here uses
a temporal split for the training and test sets: records with a CVE issued
from 2010-2014 form the training set, and those with CVE issued from 2015-2016
form the test set. (The idea, loosely, is to avoid a situation where
information from the future "leaks" into the training set.) Creating the split
is handled directly with jq and regex:

    $ cat ~/nvd-data/nvd-edb-tokens.jsonl \
        |jq -c 'select(.cveid |test("CVE-201[0-4]"))' |wc -l
      27644
    $ cat ~/nvd-data/nvd-edb-tokens.jsonl \
        |jq -c 'select(.cveid |test("CVE-201[56]"))' |wc -l
      14784

So 27.6k training examples, 14.6k test examples.

### Class imbalance

The preprocess.py script applies uniformization logic to the input records,
wherein target keys that are missing from the input are added to the output
with an empty array for their associated values. Consequently, records in the
tokenized file where "exploitdb" is a nonempty array are positive examples,
and those where it is an empty array are negative examples. Counts are
straightforward from the command line:
    $ cat ~/nvd-data/nvd-edb-tokens.jsonl \
        |jq -c 'select(.cveid |test("CVE-201[0-4]")) | (.exploitdb |length) > 0' \
        |sort |uniq -c
      26719 false
        925 true

That is, a positive class frequency of 925/26719 or about 3.5%

### Step 2. Vectorization

The bag-of-words encoder is designed to read the preprocessed feature arrays
directly from stdin, but it requires a config specifying the "dimensions"
(aka the vocabulary) of the input set. The example config
[config/baseline/features.json](config/baseline/features.json)
illustrates for the cvssV2 data, and it ignores the redundant tokens
(like "baseScore:...") from the preprocessed documents:

    $ cat ~/nvd-data/nvd-edb-tokens.jsonl \
        |jq -c 'select(.cveid |test("CVE-201[0-4]")) | .cvssV2' \
        |./vectorize.py --encoder config/baseline/features.json ~/nvd-data/training-cvssV2.npz

The output file is a scipy CSR sparse matrix.

The exact same workflow is used to generated a numpy file for the label vector.
Passing the `--labels` argument to the vectorizer tells it to emit 0/1 values
based on the "truthy" evaluation of the input array:

    $ cat ~/nvd-data/nvd-edb-tokens.jsonl \
        |jq -c 'select(.cveid |test("CVE-201[0-4]")) | .exploitdb' \
        |./vectorize.py --labels ~/nvd-data/training-exploitdb.npz

(One should verify that training-exploitdb.npz is indeed a numpy array with
925 ones and the remaining entries zeros.)

To create the test data, rerun the previous commands but modifying the
selection criterion and the destination of the files:

    $ cat ~/nvd-data/nvd-edb-tokens.jsonl |\
        jq -c 'select(.cveid |test("CVE-201[56]")) | .cvssV2' |\
        ./vectorize.py --encoder config/baseline/features.json ~/nvd-data/testing-cvssV2.npz
    $ cat ~/nvd-data/nvd-edb-tokens.jsonl |\
        jq -c 'select(.cveid |test("CVE-201[56]")) | .exploitdb' |\
        ./vectorize.py --labels ~/nvd-data/testing-exploitdb.npz


### Step 3. Evaluate a model

The respective numpy objects produced in step 2 are supplied as arguments
for fitting a scikit-learn model. As an example, the script
evaluate-logistic.py is is included for evaluating
[logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)
model.

The script takes three required arguments: features and labels files, and the
model file, the later being serialized via pickle. Running the script in
"write" mode (i.e., training the model) is handled by passing a training
config via the command line, and omitting this argument will run the script
in "read" mode, evaluating the model against the supplied features and labels.

When running in training mode, the script does not fit a `LogisticRegression`
model directly, but rather uses the `RandomizedSearchCV` driver to select the
best model via cross-validation on the penalty term. Hence the training
file is passed via the `--cv-config` argument, which expects JSON serialized
constructor arguments for the `RandomizedSearchCV` class. See the config
argument provided for details:

    $ ./evaluate-logistic.py --cv-config config/baseline/logistic.json \
        ~/nvd-data/training-cvssV2.npz \
        ~/nvd-data/training-exploitdb.npz \
        ~/nvd-data/logistic-cvssV2-exploitdb.pkl \

    $ ./evaluate-logistic.py \
        ~/nvd-data/testing-cvssV2.npz \
        ~/nvd-data/testing-exploitdb.npz \
        ~/nvd-data/logistic-cvssV2-exploitdb.pkl \

      ...INFO {'recall': 0.6486..., 'accuracy': 0.5640....,
        'precision': 0.0486...}
