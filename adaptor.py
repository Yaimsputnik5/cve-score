import json
import logging

import tensorflow as tf

LOGGER = logging.getLogger('cve-score')


def _encode_record(indices, record):
    '''Applies the encoding to a raw record.

    Arguments:
        indices:  [dict] nested structure, `{<key>: {<token>: <id>}}`, mappinng
            feature keys to look-up mappings.
        record: [dict] single JSON, which, among other properties, associates
            an array of tokens to each of the keys in `indices`.

    Returns: Dictionary associating a set of integers (token ids) to each
        of the keys in `indices`.
    '''
    return {
        key: set([
            index[token] for token in record.get(key, []) if token in index
        ])
        for (key, index) in indices.items()
    }

class JSONAdaptor(object):
    '''Adaptor class for converting stream of JSON records into a
    `tf.data.Dataset` iterator of fixed-length Tensors.

    The main assumption is the feature space is structured, mapping a (small)
    set of known keys to arrays of tokens. The arrays will be encoded via
    bag-of-words. Initialization requires a config object specifying the
    structure of the incoming records. It is a dictionary with the structure:

      target:        # keyname of target variable
      features: [
        key:         # keyname of feature variable
        vocabulary:  # list of string tokens, ie, "feature space"
      ]
    '''
    def __init__(self, config, filename):
        '''Arguments:
            config    [dict]  See class docstring.
            filename  [str]   JSON examples, one per line.
        '''
        target = config['target']
        features = config['features']
        # indices is a nested map {<feat name>: {<token>: <id>}}
        indices = {
            feature['key']: {
                token: pos for (pos, token) in enumerate(feature['vocabulary'])
            }
            for feature in features
        }

        self._features = []
        self._targets = []
        with open(filename) as fh:
            for record in map(json.loads, fh):
                self._features.append(_encode_record(indices, record))
                self._targets.append(record[target])

        LOGGER.info('read %d records from %s', len(self._features), filename)

        self._feature_sizes = [
            (key, len(index)) for (key, index) in indices.items()
        ]

    @property
    def types(self):
        '''Ouput types for use with `tf.data.Dataset.from_generator`'''
        return (
            {key: tf.float32 for (key, _) in self._feature_sizes},
            tf.int64
        )

    @property
    def shapes(self):
        '''Output shapes for use with `tf.data.Dataset.from_generator`'''
        return (
            {key: tf.TensorShape([size]) for (key, size) in
            self._feature_sizes},
            tf.TensorShape([])
        )

    @property
    def columns(self):
        return [
            tf.feature_column.numeric_column(key, shape=(size,))
            for (key, size) in self._feature_sizes
        ]

    def generator(self):
        '''Generator object for use with `tf.data.Dataset.from_generator`.

        Each item returned is a dictionary of the form `{<key>: <tensor>}`,
        where the tensors are dense 1D tensors whose shapes are defined by
        input vocabularies. (See `shapes` property.)
        '''
        dense_ids = {
            key: list(range(size)) for (key, size) in self._feature_sizes
        }
        for example_id in range(len(self._features)):
            features = {
                key: [
                    (1.0 if id_ in self._features[example_id][key] else 0.0)
                    for id_ in col_ids
                ]
                for (key, col_ids) in dense_ids.items()
            }
            yield (features, self._targets[example_id])
