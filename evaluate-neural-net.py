#!/usr/bin/env python3

import argparse
import logging
import shutil
import json

import tensorflow as tf

from decoding import get_parser
from decoding import get_feature_columns
from settings import configure_logging
from utils import write_predictions

LOGGER = logging.getLogger('cve-score')
BUFFER_SIZE = 100000  # sample size of shuffle operation


def get_batch(filename, batch_size, parser, shuffle=False):
    '''Returns a tf.data.Iterator for one epoch.

    Arguments:
        filename    [str] file of TFRecords.
        batch_size  [int] batch size per step.
        parser      [lambda] mapping `tf.Example => `(<tensors>, <tensor>)`
        shuffle     [bool] Randomize the order of examples.
    '''
    ds = tf.data.TFRecordDataset([filename])
    if shuffle:
        ds = ds.shuffle(BUFFER_SIZE)

    ds = ds.map(parser).batch(batch_size)
    iter_ = ds.make_one_shot_iterator()
    return iter_.get_next()


def get_labels(filename, parser):
    '''Returns all labels from a TFRecord file, as a list of 1D tensors
    (numpy arrays), in their order on disk.

    Arguments:
        filename [str] TFRecord file.
        parser   [lambda] Parses each TFRecord into tensors.
    '''
    results = []
    with tf.Session() as sess:
        _, labels = get_batch(filename, FLAGS.batch_size, parser)
        try:
            while True:
                 batch = sess.run(labels)
                 results.extend(list(batch))
        except tf.errors.OutOfRangeError:
            return results


def main(_):

    with open(FLAGS.config) as fh:
        config = json.load(fh)

    hidden_units = list(map(int, FLAGS.hidden_units.split(',')))
    LOGGER.info('using hidden units %s', hidden_units)
    parser = get_parser(config)

    estimator = tf.estimator.DNNClassifier(
        hidden_units,
        get_feature_columns(config),
        model_dir=FLAGS.model_dir,
        weight_column='weight',
        dropout=FLAGS.dropout,
        optimizer=tf.train.AdagradOptimizer(FLAGS.learning_rate))

    for epoch in range(FLAGS.num_epochs):
        estimator.train(
            lambda: get_batch(FLAGS.train, FLAGS.batch_size, parser, True))
        result = estimator.evaluate(
            lambda: get_batch(FLAGS.eval, FLAGS.batch_size, parser))
        LOGGER.info('%s', result)

    if FLAGS.predict_probs:
        y_hat = get_labels(FLAGS.eval, parser)
        results = estimator.predict(
            lambda: get_batch(FLAGS.eval, FLAGS.batch_size, parser),
            predict_keys=['probabilities'])
        p_hat = [item['probabilities'][1:] for item in results]

        write_predictions(FLAGS.predict_probs, p_hat, y_hat)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('config', help='JSON config for pasing examples')
    parser.add_argument('train', help='TFRecord training examples')
    parser.add_argument('eval', help='TFRecord validation examples')
    parser.add_argument('--model-dir', default='/tmp/cve-nnet')
    parser.add_argument('--hidden-units', default='128', help='comma separated')
    parser.add_argument('--learning-rate', type=float, default=1e-4)
    parser.add_argument('--dropout', type=float, default=None)
    parser.add_argument('--num-epochs', type=int, default=50)
    parser.add_argument('--batch-size', type=int, default=64)
    parser.add_argument('--logging', help='JSON logging config')
    parser.add_argument('--predict-probs', help='name of CSV to emit')
    FLAGS = parser.parse_args()
    configure_logging(FLAGS.logging)

    tf.app.run()
