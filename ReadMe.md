# Ranking software vulnerabilities

The tools here evolved in the context of exploratory analysis of software
vulnerability data. The design philosophy loosely follows that of the suite
of unix command line utilities: each script tackles a single "unit of work,"
and they are intended to be composed in a way that permits rapid iteration
of exploratory tasks.

## Software vulnerability data

On a yearly basis, thousands of software security flaws or "vulnerabilities"
are discovered, publicly reported, and patches are promptly released by the
affected vendor. An unpatched security vulnerability, however, carries only
as much risk as its likelihood being exploited, together with the exposure
entailed with a successful exploit (data theft, loss of services, etc.)
The fraction of vulnerabilities for which a reliable exploit is developed
and disseminated, either publicly or through private (criminal) exchanges,
is fairly small -- on the order of a few percent.

Because of the costs entailed continually patching and redeploying software,
the challenge to security operations teams is to assign risk to vulnerabilities
in a meaningful way, and prioritize those which a higher risk for quicker
remediation. The most widely used scoring system is
[CVSS](https://nvd.nist.gov/vuln-metrics/cvss), which assigns a score to each
vulnerability based on a vector of categorical properties and is entirely
deterministic. Research corroborates that the correlation between CVSS score
and the real likelihood of exploitability is weak.

The data used to describe software vulnerabilities is reasonably well
structured, and machine learning approaches to the problem of assigning
meaningful risk have been investigated and reported. One of the seminal
publications appeared in [KDD-2010](https://research.google.com/pubs/pub36738.html).
The intervening time since that publication has witnessed advances in the
overall quality and availability of open data around software vulnerabilities.
That same time period has not witnessed commensurate output in the number of
publications applying machine learning to the "exploit prediction" problem.
(In 2017, at the time of writing, this still feels like a green field.)
Hopefully the release of tools like this will nudge the interest of a wider
scientific audience.

## Assembling the data

In spite of the growing availability of _open data_ around software
vulnerabilities and their exploits, there still exist no public "benchmark"
datasets for evaluating and comparing machine learning techniques. These tools
aim to provide a workflow for assembling datasets and experimenting.

The canonical input for any supervised learning problem is a pair of the
form _X_, _y_, where _X_ is an N-by-D matrix of _features,_ and _y_ is an
N-dimensional vector of _labels._ Each row in the matrix is a D-dimensional
feature vector, and the corresponding entry in _y_ is its label. Among all
of the publicly available data for security vulnerability research, however,
there are really no datasets where features and useful labels are collocated.
In addition to "feature engineering," new ideas around "label engineering"
are necessary when investigating security vulnerability data, and this
generally involves merging disparate datasets together. Fortunately, the
[CVE ID](https://cve.mitre.org/) provides a stable identifier across different
datasets that can serve as a "join key" for these types of tasks.

### Tables versus JSON records

Because the target format of processed data is a matrix with _D+1_ columns,
most workflows start by marshaling the data into a tabular format at the
beginning, and performing any cleaning, transform, and normalization steps
afterwards. The DataFrame structure in R or Pandas are great tools for this.
The workflow advocated here is slightly different, and is inspired largely
by the fact that raw vulnerability data is most readily available in a
hierarchically-structured format like XML or JSON instead of flat tables.
The approach taken here is to capture raw data in files consisting of one
JSON serialized record per line, the so-called JSONL format.

The verbosity overhead for this choice of format is negligible since the
datasets under consideration are so small: 18+ years of data from the NVD
database stored as JSON records consumes about 0.5 GB storage uncompressed.
More importantly, command-line tooling for manipulating serialized JSON like
the way grep and sed are used for manipulating lines of raw text has grown
very mature with the [jq 1.5](https://stedolan.github.io/jq/) API.
