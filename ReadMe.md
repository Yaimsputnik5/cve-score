# Ranking software vulnerabilities

The tools here evolved in the context of exploratory analysis of software
vulnerability data. The design philosophy loosely follows that of the suite
of unix command line utilities: each script tackles a single "unit of work,"
and they are intended to be composed in a way that permits rapid iteration
of exploratory tasks.

## Software vulnerability data

On a yearly basis, thousands of software security flaws or "vulnerabilities"
are discovered, publicly reported, and patches are promptly released by the
affected vendor. An unpatched security vulnerability, however, carries only
as much risk as its likelihood being exploited, together with the exposure
entailed with a successful exploit (data theft, loss of services, etc.)
The fraction of vulnerabilities for which a reliable exploit is developed
and disseminated, either publicly or through private (criminal) exchanges,
is fairly small -- on the order of a few percent.

Because of the costs entailed continually patching and redeploying software,
the challenge to security operations teams is to assign risk to vulnerabilities
in a meaningful way, and prioritize those which a higher risk for quicker
remediation. The most widely used scoring system is
[CVSS](https://nvd.nist.gov/vuln-metrics/cvss), which assigns a score to each
vulnerability based on a vector of categorical properties and is entirely
deterministic. Research corroborates that the correlation between CVSS score
and the real likelihood of exploitability is weak.

The data used to describe software vulnerabilities is reasonably well
structured, and machine learning approaches vulnerability classification
have been investigated and reported. One of the seminal publications is from
[KDD-2010](https://research.google.com/pubs/pub36738.html).
The principal data source for the results that paper has since gone offline.
In the intervening time, however, the overall quality and availability of
open data round software vulnerabilities has grown. That same time period
has not witnessed commensurate growth in publications or reported advances
around the problem of vulnerability classification. One aim of the release
of these tools is to pique the interest of a wider scientific audience.

One important machine learning task (though not the only one) that these
tools aim to address is the "exploit prediction" problem. When software
vulnerabilities are documented by security researchers, they typically include
a theoretical attack scenario. Whether or not the application supported by the
software or the data that it protects is attractive enough to motivate the
emergence of a working exploit is another question. Curating datasets that
assign positive and negative labels for "exploitability" in a way that is both
systematic and convincing is just one step in approaching this problem --
call this the label engineering step. It invariably involves merging disparate
datasets together, and fortunately the [CVE ID](https://cve.mitre.org/)
provides a stable identifier across different datasets to serve as "join key."

## Assembling the data: tables versus JSON records

The canonical input format for a supervised learning problem is a pair of the
form _X_, _y_, where _X_ is an N-by-D _feature matrix,_ and _y_ is an
N-dimensional _label vector,_ representing a set of N training examples.
Because the target format for the learning method is essentially a matrix
with _D+1_ columns, machine learning workflows typically start by marshaling
the data into a tabular format, and performing any cleaning, normalization,
and feature engineering steps from there. The DataFrame structure in R or
Pandas designed around this. The workflow taken here is slightly different,
its point of departure being the fact that raw vulnerability data is most
readily available in a hierarchically structured format like XML or JSON
instead of flat tables. The idea is to work with files consisting of one JSON
record per line -- the so-called JSONL format -- for all stages of the
curation, cleaning, and feature engineering steps. The final step is to apply
a vectorizer to JSONL files (or streams) that will emit matrix and vector
objects consumable the learning method.  (Having each step produce a file
"artifact" allows for simpler pipelining, wherein any change in the desired
processing logic only requires it to resume from the last complete artifact
before the step in which the change was introduced.)

The verbosity overhead of the choice of JSONL format is negligible because
the datasets under consideration are so small: 18+ years of data from the
NVD databased serialized as JSON records consumes about 0.5 GB uncompressed.
Command line tooling for manipulating JSON has grown very mature with the
[jq 1.5](https://stedolan.github.io/jq/) API. This is to JSON what tools like
sed and grep are to manipulating lines of text, and it is the main tool that
allows the self-contained utilities here to work so well together.
