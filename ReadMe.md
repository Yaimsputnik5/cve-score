# Scoring software vulnerabilities

This project evolved as a collection of tools for analyzing software
vulnerability data. It is largely a set of command line utilities. Each
script focuses on a single "unit of work," the aim being that more complex
processing pipelines are built via composition. This design allows for
leveraging other CL utilities, keeping the surface are of the API minimal.

One of the main intended uses is training ML models for the
[exploit prediction](https://arxiv.org/abs/1707.08015) problem.
Please see that paper references for more background.

## System requirements

The utilities target Python 3 (tested against 3.5). See requirements.txt
for the Python dependencies.

[jq 1.5](https://stedolan.github.io/jq/) is required for essentially all
data processing tasks. (See **data workflow** below.) The latest stable version
for your target platform can be downloaded from its homepage; or, for Linux
systems, can be installed via the system package manager.  

## Data workflow

Exploit prediction is a supervised learning problem. Most machine learning
workflows start by marshaling the data into a tabular format--an N-by-D
feature matrix, together with an additional column for the labels--and perform
all cleaning and feature engineering steps from there. The DataFrame structure
in R or Pandas are designed around this.

The tools here emphasize a different, "opinionated" workflow whose point of
departure is the fact that raw vulnerability data is most readily available
in a hierarchically structured format like XML or JSON instead of flat tables.
The target format for the data is a file with one JSON record per line, the
so-called JSONL format. Each data cleaning or feature engineering step
consumes a JSONL file and emits a new one, thereby building a pipeline of
processing steps with checkpoints along the way.

[TensorFlow](https://www.tensorflow.org/api_docs/python/) is used for the
ML tasks. One of the utilities transforms the preprocessed JSONL files
of training and test data to TFRecord (protobuf) files.

Design choices usually trade off flexibility for simplicity. One of these
choices involves a preferred way of encoding features that is best made
explicit early on. Suppose that the input records have a top level property
called "foo," each of which is an object of categorical attributes:

    {..., "foo": {"type": "debian", "version": "1.2", "affected": true}, ...}
    {..., "foo": {"type": "legacy", ""affected": false}, ...}
    ...

One possible encoding is to create a feature for each of the paths
"foo.type", "foo.version", "foo.affected," etc., each of which will be
a categorical variable and have its own one-hot encoding. **Instead, the
preferred approach is to use a bag-of-words encoding for the top-level
property.** Its vocabulary is the space of all inter-object paths,
"type.debian", "version.1.2", "affected.true", "type.unknown", etc.

These two approaches are mathematically equivalent. However the latter
approach helps keep the data wrangling steps simpler. For each data set, one
only needs to specify the transforms and encodings for a bounded set of
top-level properties.

### Workflow outline

 1. Using tools in the `curation/` folder, create a JSONL file where each
    record contains the top-level keys of interest in their raw format.
    The values in each record can be any JSON type: strings, numbers, objects.

 2. Apply the preprocess.py script to create a new JSONL file where each
    record contains the same top-level keys, but the values are either arrays
    (of strings) for categorical features, or numeric types.

 3. Apply the encoding.py script to emit the preprocessed data in a format
    consumable for ML training and evaluation. Usually this will be
    [TFRecord](https://www.tensorflow.org/programmers_guide/datasets) files.
    Additionally one encode everything as (a pickled dictionary) of
    [scipy.sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)
    matrices or numpy vectors.
