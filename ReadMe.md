# Ranking software vulnerabilities

The tools here evolved in the context of exploratory analysis of software
vulnerability data. The design philosophy loosely follows that of the suite
of unix command line utilities: each script tackles a single "unit of work,"
and they are intended to be composed in a way that permits rapid iteration
of exploratory tasks.

## Software vulnerability data

On a yearly basis, thousands of software security flaws or "vulnerabilities"
are discovered, publicly reported, and patches are promptly released by the
affected vendor. An unpatched security vulnerability, however, carries only
as much risk as its likelihood being exploited, together with the exposure
entailed with a successful exploit (data theft, loss of services, etc.)
The fraction of vulnerabilities for which a reliable exploit is developed
and disseminated, either publicly or through private (criminal) exchanges,
is fairly small -- on the order of a few percent.

Because of the costs entailed continually patching and redeploying software,
the challenge to security operations teams is to assign risk to vulnerabilities
in a meaningful way, and prioritize those which a higher risk for quicker
remediation. The most widely used scoring system is
[CVSS](https://nvd.nist.gov/vuln-metrics/cvss), which assigns a score to each
vulnerability based on a vector of categorical properties and is entirely
deterministic. Research corroborates that the correlation between CVSS score
and the real likelihood of exploitability is weak.

The data used to describe software vulnerabilities is reasonably well
structured, and machine learning approaches to the problem of assigning
meaningful risk have been investigated and reported. One of the seminal
publications appeared in [KDD-2010](https://research.google.com/pubs/pub36738.html).
The intervening time since that publication has witnessed advances in the
overall quality and availability of open data around software vulnerabilities.
That same time period has not witnessed commensurate output in the number of
publications applying machine learning to the "exploit prediction" problem.
Hopefully the release of tools like this will nudge the interest of a wider
scientific audience.

## Assembling the data

In spite of the growing availability of _open data_ around software
vulnerabilities and their exploits, there still exist no public "benchmark"
datasets for evaluating and comparing machine learning techniques. These tools
aim to provide a workflow for assembling datasets and experimenting.

The canonical input for any supervised learning problem is a pair of the
form _X_, _y_, where _X_ is an N-by-D matrix of _features,_ and _y_ is an
N-dimensional vector of _labels._ Each row in the matrix is a D-dimensional
feature vector, and the corresponding entry in _y_ is its label. Among all
of the publicly available data for security vulnerability research, however,
there are really no datasets where features and useful labels are collocated.
In addition to "feature engineering," new ideas around "label engineering"
are necessary when investigating security vulnerability data, and this
generally involves merging disparate datasets together. Fortunately, the
[CVE ID](https://cve.mitre.org/) provides a stable identifier across different
datasets that can serve as a "join key" for these types of tasks.

### Tables versus JSON records

Because the target format of processed data is a matrix with _D+1_ columns,
most workflows start by marshaling the data into a tabular format at the
beginning, and performing any cleaning, transform, and normalization steps
afterwards. The DataFrame structure in R or Pandas are great tools for this.
The workflow advocated here is slightly different, and is inspired largely
by the fact that raw vulnerability data is most readily available in a
hierarchically-structured format like XML or JSON instead of flat tables.
The approach taken here is to capture raw data in files consisting of one
JSON record per line, the so-called JSONL format.

The verbosity overhead for this choice of format is negligible since the
datasets under consideration are so small: 18+ years of data from the NVD
database stored as JSON records consumes about 0.5 GB uncompressed. More
importantly, command-line tooling for manipulating JSON the same way grep
and sed are used for manipulating lines of raw text has grown to be very
mature with the [jq 1.5](https://stedolan.github.io/jq/) API.

## Walk-through: how well does CVSS predict exploitability?

Software vulnerabilities documented by security researchers and software
vendors typically include what could be called a "theoretical" attack scenario.
The question of exploitability -- whether an application supported by the
software or the data it protects is attractive enough to motivate an attack -- is a separate question. As a proxy for this we use the existence of an exploit
in the public [Exploit DB](https://www.exploit-db.com/).

The following experiment trains a logistic regression classifier using (parts
of) the NVD records as features, and the existence of an exploit in Exploit DB
for the positive labels. This data set exhibits strong class imbalance, as
detailed shortly.

### Step 1. Collect and clean raw features

The script fetch-nvd.sh pulls compressed JSON files from the NVD repository,
and unpacks these as a single file of JSON records. It takes a destination
path prefix as an optional argument, which defaults to $HOME/nvd-data if none
is provided.

    $ ./fetch-nvd.sh
    $ wc -l ~/nvd-data/nvd-records.jsonl
       89440 /Users/.../nvd-records.jsonl

Inspecting the few records of the file with jq reveals how deeply nested
each record is:

    $ head ~/nvd-data/nvd-records.jsonl | jq .

We use jq to prune and flatten this into a more digestible format. The
query in [clean-raw.js](config/baseline/clean-raw.js) filters on only
records that have CVSSv2 components, and keeps three main fields:

    $ cat ~/nvd-data/nvd-records.jsonl \
        | jq  -c "$(cat config/baseline/clean-raw.js)" \
        > ~/nvd-data/nvd-pruned.jsonl

### Step 2. Collect and clean raw labels

Exploit DB is maintained in a GitHub repository as a simple file tree index.
We have provided a script which naively associates CVE IDs to exploits by
string matching on the files.

    $ git clone https://github.com/offensive-security/exploit-database.git ~/nvd-data/exploit-db
    $ ./parse-cve.py ~/nvd-data/exploit-db.jsonl --exploit-db ~/nvd-data/exploit-db/platforms

The `--exploit-db` argument applies the parsing logic to that path prefix
and emits the records to the JSONL file:

    $ head ~/nvd-data/exploit-db.jsonl |jq .

It associates CVE IDs to a list of exploits in which they occur.

### Step 3. Merge the features and labels

The merge.py script is similar to the Linux join utility, except that
it operates on JSONL files and joins on a key name instead of a fixed column:

    $ ./merge.py cveid ~/nvd-data/nvd-pruned.jsonl \
        cveid ~/nvd-data/exploit-db.jsonl \
        ~/nvd-data/nvd-edb-merged.jsonl

### Feature engineering strategy

The initial release supports [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model)
encoding for "subdocuments" within the JSON records. Specifically, suppose
that the data file consists of JSON records where each has a top-level key,
`"featureX"`, mapping to an array of string tokens. Fixing a vocabulary of
size _D_, in advance, from the set of all tokens, each of the featureX
arrays becomes a _D_-dimensional (row) vector of 1s and 0s. One of the
command line utilities provided will transform the file of _N_ JSON records
into an _N-by-D_ dimensional sparse matrix (in scipy format) that can be
consumed by the training method of any scikit-learn model.

Therefore one of the main preprocessing steps involves transforming the
input set of structured JSON documents into one where the target "subdocuments"
are represented as arrays of string tokens. In this initial release, again,
the transform methods supported are:

  1. Convert nested JSON documents into arrays where each complete path
     is a token in the vocabulary. (The final element in the path will be
     string serialized if necessary.)

  2. Convert a text string into an array of tokens, split on whitespace,
     with English punctuation and stop words removed.


### Step 4. Preprocessing the features

The preprocess.py script takes as command line arguments a required config
file of preprocessing "tasks," arguments for input/output files, and an
optional filename for a JSON "summary" object.

The format of the tasks config file is an array of JSON objects, where each
specifies the "key" name of the subdocument to be transformed, and the
transform "method" to be applied. See the `METHODS` property of preprocess.py
for available methods. The [preproc.json](config/based/preproc.json) example
illustrates transforms to the data file produced in step 3: the "cvssV2"
key maps to nested JSON objects, and the "description" key maps to text
strings.

    $ ./preprocess.py config/baseline/preproc.json \
        --infile ~/nvd-data/nvd-edb-merged.jsonl \
        --outfile ~/nvd-data/nvd-edb-tokens.jsonl
    $ head ~/nvd-data/nvd-edb-tokens.jsonl |jq .


### Step 5. Vectorizing the features

To obtain a baseline performance metric, we need to select a training/test
split. Let us take all records with CVE issued from 2010-2014 for the training
set, and those with CVE issued in 2015-2016 for the test set. One can filter
these directly with jq via regex:

    $ cat ~/nvd-data/nvd-edb-tokens.jsonl |\
        jq -c 'select(.cveid |test("CVE-201[0-4]"))' |wc -l
      27644
    $ cat ~/nvd-data/nvd-edb-tokens.jsonl |\
        jq -c 'select(.cveid |test("CVE-201[56]"))' |wc -l
      14784

So 27.6k training examples, 14.6k test examples.

Recall that the existence of an exploit constitutes a positive label, which
is represented by a non-empty array for the "exploitdb" key. Again, it is
straightforward to inspect class balance from the command line:

    $ cat ~/nvd-data/nvd-edb-tokens.jsonl |\
        jq -c 'select(.cveid |test("CVE-201[0-4]")) | (.exploitdb |length) > 0' |\
        sort |uniq -c
      26719 false
        925 true

So 925 positive labels in the training set, or a positive class frequency
of about 3.5%.

The bag-of-words encoder is designed to read the preprocessed feature arrays
directly from stdin, but it requires a config specifying the "dimensions"
(aka the vocabulary) of the input set.

    $ cat ~/nvd-data/nvd-edb-tokens.jsonl |\
        jq -c 'select(.cveid |test("CVE-201[0-4]")) | .cvssV2' |\
        ./vectorize.py --encoder config/baseline/features.json ~/nvd-data/training-cvssV2.npz

Note that the dimensions specified in features.json restricts to a smaller
18-dimensional subset of those that actually occur. Most of the properties
included corresponding subdocuments for the raw NVD data are redundant.

Using the `--labels` argument, the vectorizer script will emit 0/1 values
based on the "truthy" evaluation of the input array:

    $ cat ~/nvd-data/nvd-edb-tokens.jsonl |\
        jq -c 'select(.cveid |test("CVE-201[0-4]")) | .exploitdb' |\
        ./vectorize.py --labels ~/nvd-data/training-exploitdb.npz

One should be able to verify (for instance, within the python interpreter) that
training-exploitdb.npz contains a 27604-dimensional vector with 923 ones and
the remaining entries zeros.

To create the test data, rerun the previous commands but modifying the
selection criterion and the destination of the files:

    $ cat ~/nvd-data/nvd-edb-tokens.jsonl |\
        jq -c 'select(.cveid |test("CVE-201[56]")) | .cvssV2' |\
        ./vectorize.py --encoder config/baseline/features.json ~/nvd-data/testing-cvssV2.npz
    $ cat ~/nvd-data/nvd-edb-tokens.jsonl |\
        jq -c 'select(.cveid |test("CVE-201[56]")) | .exploitdb' |\
        ./vectorize.py --labels ~/nvd-data/testing-exploitdb.npz


### Step 6. Evaluate a model

The respective numpy objects of features and labels (i.e., training-cvssV2.npz
and training-exploitdb.npz) are the arguments for fitting a scikit-learn model.
As an example, the script evaluate-logistic.py is is included for evaluating
a [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)
model.

The script takes three required arguments: features and labels files (both
numpy format), and "model" file, which expects the pickle format from the
[joblib](http://pythonhosted.org/joblib/) library. Whether the script writes
(ie trains) the model versus reads (ie tests) it is determined by passing
the `--cv-config` argument. The script does not train a scikit-learn
`LogisticRegression` model directly, but rather uses the `RandomizedSearchCV`
driver select the best model via cross-validation on the penalty
hyper-parameter. The `--cv-config` argument passes a JSON file which, when
deserialized, serves as the constructor arguments for the `RandomizedSearchCV`
class, and initializes the training step. Omitting this argument loads the
specified model (pickle file) and, emits predictions against the features
vectors, and reports performance metrics against the specified labels.

    $ ./evaluate-logistic.py --cv-config config/baseline/logistic.json \
        ~/nvd-data/training-cvssV2.npz \
        ~/nvd-data/training-exploitdb.npz \
        ~/nvd-data/logistic-cvssV2-exploitdb.pkl \

    $ ./evaluate-logistic.py \
        ~/nvd-data/testing-cvssV2.npz \
        ~/nvd-data/testing-exploitdb.npz \
        ~/nvd-data/logistic-cvssV2-exploitdb.pkl \

      ...INFO {'recall': 0.6486..., 'accuracy': 0.5640....,
        'precision': 0.0486...}
