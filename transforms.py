import re


def flatten(document):
    '''Flattens a nested "document" of dictonaries and lists into a single
    list of `<path>:<leaf>` strings.
    '''
    state = []
    def recur(key, value):
        # Unpacks the `value` object to recursively add keys to the state
        if isinstance(value, dict):
            for child, new_value in value.items():
                new_key = '.'.join(filter(None, [key, child]))
                recur(new_key, new_value)
        elif isinstance(value, list):
            for child in value:
                recur(key, child)
        else:
            state.append(':'.join(filter(None, [key, str(value)])))

    if isinstance(document, dict):
        for key, value in document.items():
            recur(key, value)
    else:
        recur('', document)

    return state


def strip_punctuation(token):
    '''Returns the token with basic English punctuation removed.'''
    return re.sub('^\W*', '', re.sub('\W*$', '', token))

# English stopwords from <http://tm.r-forge.r-project.org/>
# License: GLP-3
_tm_stopwords = {
    "i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you",
    "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself",
    "she", "her", "hers", "herself", "it", "its", "itself", "they", "them",
    "their", "theirs", "themselves", "what", "which", "who", "whom", "this",
    "that", "these", "those", "am", "is", "are", "was", "were", "be", "been",
    "being", "have", "has", "had", "having", "do", "does", "did", "doing",
    "would", "should", "could", "ought", "i'm", "you're", "he's", "she's",
    "it's", "we're", "they're", "i've", "you've", "we've", "they've", "i'd",
    "you'd", "he'd", "she'd", "we'd", "they'd", "i'll", "you'll", "he'll",
    "she'll", "we'll", "they'll", "isn't", "aren't", "wasn't", "weren't",
    "hasn't", "haven't", "hadn't", "doesn't", "don't", "didn't", "won't",
    "wouldn't", "shan't", "shouldn't", "can't", "cannot", "couldn't",
    "mustn't", "let's", "that's", "who's", "what's", "here's", "there's",
    "when's", "where's", "why's", "how's", "a", "an", "the", "and", "but",
    "if", "or", "because", "as", "until", "while", "of", "at", "by", "for",
    "with", "about", "against", "between", "into", "through", "during",
    "before", "after", "above", "below", "to", "from", "up", "down", "in",
    "out", "on", "off", "over", "under", "again", "further", "then", "once",
    "here", "there", "when", "where", "why", "how", "all", "any", "both",
    "each", "few", "more", "most", "other", "some", "such", "no", "nor",
    "not", "only", "own", "same", "so", "than", "too", "very",
}

def strip_stopwords(token):
    '''Returns the empty string if the token is a (lower case) English stop
    word; otherwise returns the token.
    '''
    return '' if token in _tm_stopwords else token

def tokenize(document):
    '''Tokenizes a (string) document into a list of unigrams:
    (1) normalizes case,
    (2) removes punctuation and stopwords.
    '''
    return list(
        filter(None,
        map(strip_stopwords,
        map(strip_punctuation,
        re.split('\s+', document.lower())))))
