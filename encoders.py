import numpy as np
from scipy import sparse
from scipy.linalg import eigh


def trim_vocabulary(
        vocabulary, vocabulary_max_size=None, vocabulary_min_freq=None):
    '''Extracts tokens from a vocabulary dictionary subject to size or
    frequency constraints.

    Arguments:
        vocabulary: [dict] mapping tokens to frequencies.
        vocabulary_max_size: [int|None] limit on the absolute size of the
            emitted vocabulary.
        vocabulary_min_freq: [int|None] include only tokens whose
            corresponding frequency is at least this value.

    Returns a list of tokens.
    '''
    max_size = vocabulary_max_size or len(vocabulary)
    min_freq = vocabulary_min_freq or 1
    if isinstance(vocabulary, dict):
        freq_token_pairs = [
            (freq, token) for (token, freq) in vocabulary.items()
            if freq >= min_freq
        ]
        freq_token_pairs.sort(reverse=True)
        return [token for (_, token) in freq_token_pairs[:max_size]]
    raise ValueError('"vocabulary" must be a dict.')


class NumericEncoder(object):
    '''Implements a "pass through" encoder for creating 1D tensors.'''

    def __init__(self, dtype='int32', **kwargs):
        self._dtype = np.dtype(dtype)

    def __call__(self, value):
        return value

    def transform(self, values):
        return np.array(values, dtype=self._dtype)


class _BoWEncoder(object):
    '''Base class for encoding tokens as integers. Initializes its state with
    an `_index` object, that maps the space of tokens to integer indices.
    '''
    def __init__(self,
            vocabulary,
            vocabulary_max_size=None,
            vocabulary_min_freq=None, **kwargs):
        trimmed = trim_vocabulary(
            vocabulary, vocabulary_max_size, vocabulary_min_freq)
        self._index = {token: i for (i, token) in enumerate(trimmed)}
        self._input_dim = len(self._index)

    def __call__(self, tokens):
        return [
            self._index[token] for token in tokens
            if token in self._index
        ]

    def transform(self, records):
        raise NotImplementedError()


class DenseEncoder(_BoWEncoder):
    '''Token encoder whose `transform` method returns a dense NumPy array.'''

    def __init__(self,
            vocabulary,
            vocabulary_max_size=None,
            vocabulary_min_freq=None, **kwargs):
        '''Arguments:
            vocabulary: dictionary mapping tokens to [int|float]
                frequency counts.
            vocabulary_max_size: [int|None] if provided, restricts to only
                the set of most frequent tokens meeting this size.
            vocabulary_min_freq: [int|None] if provided, restricts to only
                the set of tokens whose frequency is at least this value.
        '''
        super().__init__(vocabulary, vocabulary_max_size, vocabulary_min_freq)

    def transform(self, records):
        '''Transforms a list of encoded records into a 2D NumPy tensor.'''
        shape = len(records), self._input_dim
        matrix = np.empty(shape, dtype=np.float32)
        for row, indices in enumerate(records):
            matrix[row, indices] = 1
        return matrix


class SparseEncoder(_BoWEncoder):
    '''Token encoder whose `transform` method returns a SciPy sparse array.'''

    def __init__(self,
            vocabulary,
            vocabulary_max_size=None,
            vocabulary_min_freq=None, **kwargs):
        '''Arguments:
            vocabulary: dictionary mapping tokens to [int|float]
                frequency counts.
            vocabulary_max_size: [int|None] if provided, restricts to only
                the set of most frequent tokens meeting this size.
            vocabulary_min_freq: [int|None] if provided, restricts to only
                the set of tokens whose frequency is at least this value.
        '''
        super().__init__(vocabulary, vocabulary_max_size, vocabulary_min_freq)

    def transform(self, records):
        '''Transforms a list of encoded records into a 2D sparse tensor.'''
        shape = len(records), self._input_dim
        row_ind, col_ind = [], []
        for row, indices in enumerate(records):
            col_ind.extend(indices)
            row_ind.extend([row] * len(indices))

        return sparse.csr_matrix(
            ([1] * len(row_ind), (row_ind, col_ind)),
            shape=shape,
            dtype=np.float32)


class TfEmbeddingEncoder(object):
    '''Token encoder whose `transform` method returns NumPy array of
    integer indices compatible with TensorFlow Embedding layers.
    '''

    def __init__(self,
            vocabulary,
            input_length,
            vocabulary_max_size=None,
            vocabulary_min_freq=None, **kwargs):
        '''Arguments:
            vocabulary: dictionary mapping tokens to [int|float]
                frequency counts.
            input_length: [int] truncates all input documents to include
                at most this many tokens.
            vocabulary_max_size: [int|None] if provided, restricts to only
                the set of most frequent tokens meeting this size.
            vocabulary_min_freq: [int|None] if provided, restricts to only
                the set of tokens whose frequency is at least this value.
        '''
        trimmed = trim_vocabulary(
            vocabulary, vocabulary_max_size, vocabulary_min_freq)
        # uses index 0 for padding
        self._index = {token: i + 1 for (i, token) in enumerate(trimmed)}
        self._input_dim = len(self._index) + 1
        self._input_length = int(input_length)

    def __call__(self, tokens):
        return [
            self._index[token] for token in tokens
            if token in self._index
        ]

    def transform(self, records):
        '''Transforms a list of encoded records into a 2D NumPy tensor of
        integer indices with padding.
        '''
        shape = len(records), self._input_length
        # this is where 0-padding is enforceds
        matrix = np.zeros(shape, dtype=np.int32)
        for row, indices in enumerate(records):
            row_size = min(self._input_length, len(indices))
            matrix[row, 0:row_size] = indices[:row_size]
        return matrix


class SifEmbeddingEncoder(_BoWEncoder):
    '''Encoder using the Smooth Inverse Frequency weighting scheme,

        @article{sif2017,
            author = {Sanjeev Arora and Yingyu Liang and Tengyu Ma},
            title = {A Simple but Tough-to-Beat Baseline for Sentence
                Embeddings},
            booktitle = {International Conference on Learning Representations},
            year = {2017}
        }
    '''
    def __init__(self, vocabulary, embedding, sif_a, **kwargs):
        '''Arguments:
            vocabulary: dictionary mapping tokens to [int|float]
                frequency counts.
            embedding: dict mappping tokens to 1D NumPy tensors of
                fixed dimension.
            sif_a: [float] smoothing parameter from cited paper.

        Note that the key sets of `vocabulary` and `embedding` need not
        conicide. However only the intersection of these sets will be
        used to assemble the internal representation.
        '''
        # First project the vocabulary onto the token space of the embedding.
        vocabulary = {
            token: rank for (token, rank) in vocabulary.items()
            if token in embedding
        }
        super().__init__(vocabulary, vocabulary_min_freq=0)
        # The input records are lists of integer indices. Embedding vectors
        # are (1) preprocessed with weights, and (2) organized to be indexed
        # directly.
        embedding_dim = next(iter(embedding.values())).shape[0]
        self._word_vecs = np.empty(
            (self._input_dim, embedding_dim), dtype=np.float32)
        corpus_size = sum(vocabulary.values())
        for (token, row) in self._index.items():
            weight = sif_a / (sif_a + vocabulary[token] / corpus_size)
            self._word_vecs[row, :] = weight * embedding[token]

    def transform(self, records):
        '''Transforms a list of encoded records into a 2D sparse tensor.'''
        # Preprocessing word vectors allows semantic vectors to be computed
        # as the sum of a "bag of vectors."
        doc_vecs = np.vstack([
            np.sum(self._word_vecs[indices, :], axis=0) for indices in records
        ])
        # Extract the principal component of the "correlation" matrix:
        inner_prod = np.matmul(np.transpose(doc_vecs), doc_vecs)
        dim = inner_prod.shape[0]
        _, prin_component = eigh(inner_prod, eigvals=(dim - 1, dim - 1))
        prin_component = prin_component.reshape((dim,))
        # And remove that part of the component from each semantic vector:
        projections = np.matmul(doc_vecs, prin_component)
        for row in range(doc_vecs.shape[0]):
            doc_vecs[row, :] -= projections[row] * prin_component

        return doc_vecs
