#!/usr/bin/env python3

import argparse
import joblib
import json
import logging

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models

from common import deserialize, serialize
from encode import META_KEY
import settings


_PARSER = argparse.ArgumentParser()
_PARSER.add_argument('command', help='train|eval')
_PARSER.add_argument('dataset', help='serialized dict of numpy arrays')
_PARSER.add_argument('estimator', help='serialized sklearn estimator')
_PARSER.add_argument('--feature-keys', nargs='*',
    help='feature key in dataset')
_PARSER.add_argument('--label-key', help='label key in dataset')
_PARSER.add_argument('--batch-size', type=int, default=32,
    help='keras.models.Model#fit argument')
_PARSER.add_argument('--epochs', type=int, default=10,
    help='keras.models.Model#fit argument')
_PARSER.add_argument('--validation-split', type=float, default=0.25,
    help='keras.models.Model#fit argument')

LOGGER = logging.getLogger('cve-score')

OUTPUT_DIM = 128
KERAS_METRICS = [
    tf.keras.metrics.Precision(),
    tf.keras.metrics.Recall(),
]


def _input_layer(key, tensor):
    input_dim = int(tensor.shape[-1])
    return layers.Input(shape=(input_dim,), name=key)


def _embedding_layer(input_layer, key, tensor):
    if tensor.dtype != 'int32':
        return input_layer
    input_dim = np.max(tensor) + 1
    input_length = tensor.shape[-1]
    embedding = layers.Embedding(
        input_dim, OUTPUT_DIM, input_length=input_length)(input_layer)
    return layers.Flatten()(embedding)


def _class_weights(labels):
    '''Implements `sklearn.utils.class_weight` method.'''
    n_samples = labels.shape[0]
    weights = n_samples / 2 / np.bincount(labels)
    return dict(zip(range(2), weights))


def get_model(features):
    '''Returns a keras.Model.'''
    features = list(features.items())   # fix the ordering
    inputs = [
        _input_layer(key, tensor) for (key, tensor) in features
    ]
    embeddings = [
        _embedding_layer(input, key, tensor)
        for (input, (key, tensor)) in zip(inputs, features)
    ]
    concatenated = (embeddings[0] if len(embeddings) == 1
        else layers.concatenate(embeddings))

    outputs = layers.Dense(1, activation=tf.nn.softmax)(concatenated)

    model = models.Model(inputs=inputs, outputs=outputs)
    model.compile('adam', loss='binary_crossentropy', metrics=KERAS_METRICS)
    LOGGER.info('model => %s', model.get_config())
    return model


def train(
    dataset, estimator, feature_keys, label_key, **kwargs):
    data = deserialize(dataset)
    features = {
        key: tensor for (key, tensor) in data.items()
        if key in feature_keys
    }
    labels = data[label_key]
    model = get_model(features)
    params = {
        key: value for (key, value) in kwargs.items()
        if key in {'batch_size', 'epochs', 'validation_split'}
    }
    params['class_weight'] = _class_weights(labels)
    model.fit(features, labels, **params)


def eval(dataset, estimator, feature_keys, label_key, **kwargs):
    pass

if __name__ == '__main__':
    args = _PARSER.parse_args()
    settings.configure_logging()

    if args.command == 'train':
        train(**vars(args))
    elif args.command == 'eval':
        eval(**vars(args))
    else:
        print('invalid command: %s' % args.command)
