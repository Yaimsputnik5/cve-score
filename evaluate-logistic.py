#!/usr/bin/env python3

import json
import logging
import argparse
import utils
import settings
import joblib
import numpy as np
from scipy import stats
from sklearn import model_selection
from sklearn import linear_model

def load_estimator(config=None, filename=None):
    '''Initializes sklearn.LogisticRegression object.

    Arguments:
        config:     [dict] constructor arguments for LogisticRegression.
        filename:   [str] file with `joblib` serialized model.

    First checks whether `config` is provided, in which case the initialized
    class is returned. Otherwise, the object rehydrated from the serialized
    file is returned.
    '''
    if config:
        logging.info('LogisticRegression << %s', config)
        return linear_model.LogisticRegression(**config)
    elif filename:
        logging.info('loading %s', filename)
        return joblib.load(fh)

def build_evaulator(config, args):
    '''Initializes a sklearn.RandomizedSearchCV object for hyper-parameter
    optimization.

    Arguments:
        config:  [dict] constructor arguments (see below)
        args:    [argparse.Namespace] contains CL arguments.

    The config argument expects an "estimator" key, which holds the constructor
    arguments for sklearn.LogisticRegression. The "param_distributions" key,
    if passed, is ignored.
    '''
    model_cfg = config['estimator']
    config = config.copy()  # push mutable arguments out of scope
    config['estimator'] = load_estimator(config=model_cfg)
    config['param_distributions'] = {
        'C': stats.expon(scale=args.l2scale)
    }
    config['random_state'] = args.random_state
    logging.info('RandomizedSearchCV << %s', config)
    return model_selection.RandomizedSearchCV(**config)

def train_model(args):
    data = utils.load_data(vars(args))
    cv_config = json.load(open(args.cv_config))
    evaluator = build_evaulator(cv_config, args)
    evaluator.fit(data.features, data.labels)
    logging.info('best params => %s', evaluator.best_params_)
    with open(args.model, 'wb') as fh:
        logging.info('writing %s', args.model)
        joblib.dump(evaluator.best_estimator_, fh)

def evaluate_model(args):
    data = utils.load_data(vars(args))
    logging.info('loading %s', args.model)
    estimator = joblib.load(args.model)
    y_predict = estimator.predict(data.features)
    y_probs = estimator.predict_proba(data.features)
    results = dict(
        scores=utils.scores(data.labels, y_predict),
        probabilities=utils.probabilities(
            data.labels, y_probs, estimator.classes_)
    )
    with open(args.results, 'w') as fh:
        logging.info('writin %s', args.results)
        json.dump(results, fh)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('features', help='sparse matrix file, npz format')
    parser.add_argument('labels', help='label vector file, numpy-serialized')
    parser.add_argument('model', help='trained model "pickle" file')
    parser.add_argument('--cv-config',
        help='JSON file for cross-validation config')
    parser.add_argument('--results', help='emit evaluation stats, JSON format')
    parser.add_argument('--l2scale', type=float, default=20,
        help='scipy.stats.expon parameter, for hyper-param sampling')
    parser.add_argument('--random-state', type=int, default=1337)
    parser.add_argument('--logging', help='path to JSON config')
    args = parser.parse_args()
    settings.configure_logging(args.logging)

    if args.cv_config:
        train_model(args)
    elif args.results:
        evaluate_model(args)
