#!/usr/bin/env python3

import json
import logging
import argparse
import data_emitter
import settings
import utils
import numpy as np
from scipy import stats
from sklearn import model_selection
from sklearn import linear_model


def build_evaulator(config, args):
    '''Assembles the evaluator object for random search cross-validation.
    The `config` argument is expected to have the following structure:

        config = {
            'estimator': {
                'args': // constructor args for LogisticRegression
            }
            'param_distributions': {
                // any keys for LogisticRegression, but 'C' will be
                // replaced by an initialized object.
            }
        }

    Returns: sklearn.model_selection.RandomizedSearchCV
    '''
    estimator_args = config['estimator'].get('args', {})
    param_distributions = config.get('param_distributions', {})
    param_distributions['C'] = stats.expon(scale=args.l2scale)

    estimator = linear_model.LogisticRegression(**estimator_args)
    logging.info('LogisticRegression << %s', estimator_args)

    config = config.copy()  # push mutable arguments out of scope
    config['estimator'] = estimator
    config['param_distributions'] = param_distributions
    evaluator = model_selection.RandomizedSearchCV(**config)
    logging.info('RandomizedSearchCV << %s', config)
    return evaluator

def main_loop(data, evaluator, args):
    for iter_ in range(1, args.num_iter+1):
        X_train, X_test, y_train, y_test = data.split(args.test_size)
        evaluator.fit(X_train, y_train)
        logging.info('iter %d params %s', iter_, evaluator.best_params_)
        scores = utils.scores(y_test, evaluator.predict(X_test))
        logging.info('iter %d scores %s', iter_, scores)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('config', help='JSON file with CV config')
    parser.add_argument('features', help='sparse matrix file, npz format')
    parser.add_argument('labels', help='label vector file, numpy-serialized')
    parser.add_argument('--l2scale', type=float, default=20,
        help='scipy.stats.expon parameter, for hyper-param sampling')
    parser.add_argument('--random-state', type=int, default=1337)
    parser.add_argument('--test-size', type=float, default=0.2,
        help='[float] rate of items in test/train split')
    parser.add_argument('--num-iter', type=int, default=5,
        help='[int] number of split/train/eval cycles')
    parser.add_argument('--logging', help='path to JSON config')
    args = parser.parse_args()
    settings.configure_logging(args.logging)

    config = json.load(open(args.config))
    evaluator = build_evaulator(config, args)
    data = data_emitter.DataEmitter(args.features, args.labels,
        random_state=args.random_state)

    main_loop(data, evaluator, args)
